"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤–∞—Ä–µ–π —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤
–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
1. danakt/russian-words - –≤—Å–µ —Å–ª–æ–≤–∞
2. Harrix/Russian-Nouns - —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
"""

import requests
import json
import re
from typing import List, Set

def download_all_russian_words() -> Set[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –≤—Å–µ—Ö —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤ (1.5M)"""
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã russian-words (1.5M —Å–ª–æ–≤)...")
    
    url = "https://raw.githubusercontent.com/danakt/russian-words/master/russian.txt"
    
    try:
        response = requests.get(url, timeout=30)
        response.encoding = 'utf-8'
        
        words = set()
        for line in response.text.split('\n'):
            word = line.strip().lower()
            # –§–∏–ª—å—Ç—Ä—É–µ–º: —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã, –¥–ª–∏–Ω–∞ 3-20 —Å–∏–º–≤–æ–ª–æ–≤
            if word and re.match(r'^[–∞-—è—ë]{3,20}$', word):
                words.add(word)
        
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(words)} —Å–ª–æ–≤ –∏–∑ russian-words")
        return words
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return set()

def download_russian_nouns() -> Set[str]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ (125K)"""
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö Russian-Nouns...")
    
    url = "https://raw.githubusercontent.com/Harrix/Russian-Nouns/master/dist/russian_nouns.txt"
    
    try:
        response = requests.get(url, timeout=30)
        response.encoding = 'utf-8'
        
        nouns = set()
        for line in response.text.split('\n'):
            word = line.strip().lower()
            if word and re.match(r'^[–∞-—è—ë]{3,20}$', word):
                nouns.add(word)
        
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(nouns)} —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö")
        return nouns
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return set()

def merge_dictionaries(all_words: Set[str], nouns: Set[str]) -> List[str]:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —ë ‚Üí –µ"""
    print("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π...")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ (—ë ‚Üí –µ)
    normalized_nouns = set()
    for word in nouns:
        normalized = word.replace('—ë', '–µ')
        normalized_nouns.add(normalized)
    
    final_words = set(normalized_nouns)
    
    noun_like_endings = ['–∞', '—è', '–æ', '–µ', '—å', '–π', '–∏–µ', '–∏—è', '–∫–∞', '–æ–∫', '–µ–∫', '–∏–∫']
    
    for word in all_words:
        normalized = word.replace('—ë', '–µ')
        if any(normalized.endswith(ending) for ending in noun_like_endings):
            if not (normalized.endswith('—Ç—å') or normalized.endswith('—Ç–∏')):
                final_words.add(normalized)
    
    result = sorted(list(final_words))
    print(f"‚úì –ò—Ç–æ–≥–æ —Å–ª–æ–≤: {len(result)}")
    return result


def create_word_database(words: List[str]) -> dict:
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤"""
    print("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    
    word_db = {}
    
    for idx, word in enumerate(words):
        word_db[word] = {
            'id': idx,
            'word': word,
            'length': len(word),
            'first_letter': word[0],
            'last_letter': word[-1],
            'rank': 99999,  # –ù–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–Ω–≥ –¥–ª—è –Ω–µ–∑–Ω–∞–∫–æ–º—ã—Ö —Å–ª–æ–≤
            'frequency': 0,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–≥—Ä—ã
            'times_guessed': 0,
            'times_used_as_target': 0
        }
    
    print(f"‚úì –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞: {len(word_db)} —Å–ª–æ–≤")
    return word_db

def save_database(word_db: dict, filename: str = 'word_database.json'):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –±–∞–∑—É –≤ JSON"""
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {filename}...")
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(word_db, f, ensure_ascii=False, indent=2)
    
    file_size = len(json.dumps(word_db)) / 1024 / 1024
    print(f"‚úì –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! –†–∞–∑–º–µ—Ä: {file_size:.2f} MB")

def create_compact_version(word_db: dict, filename: str = 'words_compact.json'):
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –≤–µ—Ä—Å–∏—é (—Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤)"""
    print(f"üíæ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–π –≤–µ—Ä—Å–∏–∏...")
    
    words_list = list(word_db.keys())
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(words_list, f, ensure_ascii=False)
    
    print(f"‚úì –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞: {len(words_list)} —Å–ª–æ–≤")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 60)
    print("üéÆ WORDWEAVE - –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã —Å–ª–æ–≤")
    print("=" * 60)
    
    # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–∞ —Å–ª–æ–≤–∞—Ä—è
    all_words = download_all_russian_words()
    nouns = download_russian_nouns()
    
    if not all_words and not nouns:
        print("‚ùå –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ª–æ–≤–∞—Ä–∏!")
        return
    
    # –®–∞–≥ 2: –û–±—ä–µ–¥–∏–Ω—è–µ–º
    final_words = merge_dictionaries(all_words, nouns)
    
    # –®–∞–≥ 3: –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    word_db = create_word_database(final_words)
    
    # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω—è–µ–º
    save_database(word_db, 'word_database.json')
    create_compact_version(word_db, 'words_compact.json')
    
    print("\n" + "=" * 60)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 60)
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –í—Å–µ–≥–æ —Å–ª–æ–≤: {len(word_db)}")
    print(f"   - –§–∞–π–ª –±–∞–∑—ã: word_database.json")
    print(f"   - –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π: words_compact.json")
    print("=" * 60)

if __name__ == "__main__":
    main()
